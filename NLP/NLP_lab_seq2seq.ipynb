{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrMiracle/Colab-Projects/blob/main/NLP/NLP_lab_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJHdUAy3He0r",
        "outputId": "dfbf8f1b-1d64-40e9-8923-349ee89c6c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Style-Transfer'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Total 11 (delta 0), reused 0 (delta 0), pack-reused 11 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (11/11), 1.62 KiB | 332.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/datasets/RUCAIBox/Style-Transfer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf Style-Transfer/gyafc_em.tgz\n",
        "!tar -xvzf Style-Transfer/gyafc_fr.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2z-my4vHl2F",
        "outputId": "0314fb3b-fbc5-464d-ac80-043d9b453f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gyafc_em/\n",
            "gyafc_em/valid.src\n",
            "gyafc_em/test.tgt\n",
            "gyafc_em/train.tgt\n",
            "gyafc_em/train.src\n",
            "gyafc_em/.DS_Store\n",
            "gyafc_em/valid.tgt\n",
            "gyafc_em/test.src\n",
            "gyafc_fr/\n",
            "gyafc_fr/valid.src\n",
            "gyafc_fr/test.tgt\n",
            "gyafc_fr/train.tgt\n",
            "gyafc_fr/train.src\n",
            "gyafc_fr/.DS_Store\n",
            "gyafc_fr/valid.tgt\n",
            "gyafc_fr/test.src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"gyafc_fr/train.src\", \"r\") as f:\n",
        "  print(\"Input:\")\n",
        "  print(f.readline())\n",
        "\n",
        "with open(\"gyafc_fr/train.tgt\", \"r\") as f:\n",
        "  print(\"Target:\")\n",
        "  print(f.readline())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NE1z7OXwI4XZ",
        "outputId": "72d50b61-2fba-4f7e-e319-aec3041f1058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "Sure, it's ok, but I always have let the guy ask me.\n",
            "\n",
            "Target:\n",
            "I prefer to let the guy ask me.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "metadata": {
        "id": "tS-4QDfq2pzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "AMnnmAoUUcHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", bos_token = \"<s>\")\n",
        "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"<s> $A </s>\",\n",
        "    pair=\"<s> $A </s> <s> $B </s>\",\n",
        "    special_tokens=[(\"</s>\", tokenizer.eos_token_id), (\"<s>\", tokenizer.bos_token_id)]\n",
        ")\n",
        "BOS_INDEX = tokenizer.bos_token_id\n",
        "PAD_INDEX = tokenizer.pad_token_id\n",
        "EOS_INDEX = tokenizer.eos_token_id\n",
        "VOCAB_SIZE = tokenizer.vocab_size+1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPIxH_K1emfn",
        "outputId": "76a7cfee-ab9c-45f1-857a-80c3c2bc067c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN_SIZE = 512\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "MAX_LENGTH = 50\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "GNRL52dpUde1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataframe(source, target, train = True):\n",
        "    df = []\n",
        "    with open(source, \"r\") as src, open(target, \"r\") as tgt:\n",
        "        for s, t in zip(src, tgt):\n",
        "            df.append({\n",
        "                \"source\": s,\n",
        "                \"target\": t if train else eval(t)\n",
        "            })\n",
        "    return pd.DataFrame(df)"
      ],
      "metadata": {
        "id": "h5t1B2glU7Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, train = True):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.train = train\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataframe.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = self.tokenizer(self.dataframe['source'][idx],\n",
        "                                   max_length=MAX_LENGTH,\n",
        "                                   padding=\"max_length\",\n",
        "                                   truncation=True,\n",
        "                                   return_tensors=\"pt\")[\"input_ids\"].squeeze()\n",
        "\n",
        "        target_ids = self.tokenizer(self.dataframe['target'][idx],\n",
        "                                        max_length=MAX_LENGTH,\n",
        "                                        padding=\"max_length\",\n",
        "                                        truncation=True,\n",
        "                                        return_tensors=\"pt\")[\"input_ids\"].squeeze()\n",
        "        return input_ids, target_ids\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = Attention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(BOS_INDEX)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights\n"
      ],
      "metadata": {
        "id": "Fhjde6KSUMX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for input, target in tqdm(dataloader):\n",
        "        input, target = input.to(device), target.to(device)\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "_MhV_qZPiYnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bleu(dataloader, encoder, decoder):\n",
        "    total_bleu = 0\n",
        "    with torch.no_grad():\n",
        "        for input, target in tqdm(dataloader):\n",
        "            input, target = input.to(device), target.to(device)\n",
        "\n",
        "            encoder_outputs, encoder_hidden = encoder(input)\n",
        "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "            _, topi = decoder_outputs.topk(1)\n",
        "            decoded_ids = topi.reshape(input.shape[0], -1)\n",
        "\n",
        "            hypothesis = [tokenizer.decode(ids) for ids in decoded_ids]\n",
        "            references = [[tokenizer.decode(t) for t in l_t] for l_t in target]\n",
        "\n",
        "            total_bleu += corpus_bleu(references, hypothesis)\n",
        "\n",
        "    return total_bleu / len(dataloader)"
      ],
      "metadata": {
        "id": "fguBTukaylHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = read_dataframe(\"gyafc_em/train.src\", \"gyafc_em/train.tgt\")\n",
        "valid_df = read_dataframe(\"gyafc_em/valid.src\", \"gyafc_em/valid.tgt\", train = False)\n",
        "test_df = read_dataframe(\"gyafc_em/test.src\", \"gyafc_em/test.tgt\", train = False)\n",
        "\n",
        "train_dataset = TextDataset(train_df, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "valid_dataset = TextDataset(valid_df, tokenizer)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "encoder = Encoder(VOCAB_SIZE, HIDDEN_SIZE).to(device)\n",
        "decoder = Decoder(HIDDEN_SIZE, VOCAB_SIZE).to(device)\n",
        "\n",
        "en_optimizer = optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
        "de_optimizer = optim.Adam(decoder.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.NLLLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "losses = []\n",
        "bleus = []\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_loss = train_epoch(train_loader, encoder, decoder, en_optimizer, de_optimizer, criterion)\n",
        "    bleu = evaluate_bleu(valid_loader, encoder, decoder)\n",
        "\n",
        "    losses.append(train_loss)\n",
        "    bleus.append(bleu)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, Bleu: {bleu:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNWSh3GVWdQx",
        "outputId": "6ed9b618-e76d-444d-bc30-deec77e5ab47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 411/411 [04:44<00:00,  1.45it/s]\n",
            "100%|██████████| 23/23 [00:14<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 4.6641, Bleu: 0.1197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 411/411 [04:44<00:00,  1.45it/s]\n",
            "100%|██████████| 23/23 [00:13<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Train Loss: 3.4419, Bleu: 0.1512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 411/411 [04:45<00:00,  1.44it/s]\n",
            "100%|██████████| 23/23 [00:14<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Train Loss: 2.8031, Bleu: 0.1746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 411/411 [04:46<00:00,  1.44it/s]\n",
            "100%|██████████| 23/23 [00:14<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Train Loss: 2.3385, Bleu: 0.1865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 411/411 [04:44<00:00,  1.44it/s]\n",
            "100%|██████████| 23/23 [00:14<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Train Loss: 1.9859, Bleu: 0.1921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 411/411 [04:45<00:00,  1.44it/s]\n",
            "100%|██████████| 23/23 [00:14<00:00,  1.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Train Loss: 1.7179, Bleu: 0.1998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 411/411 [04:44<00:00,  1.44it/s]\n",
            "100%|██████████| 23/23 [00:14<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Train Loss: 1.5145, Bleu: 0.1968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 411/411 [04:44<00:00,  1.44it/s]\n",
            "100%|██████████| 23/23 [00:14<00:00,  1.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Train Loss: 1.3551, Bleu: 0.2047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 411/411 [04:44<00:00,  1.45it/s]\n",
            "100%|██████████| 23/23 [00:14<00:00,  1.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Train Loss: 1.2293, Bleu: 0.2058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 411/411 [04:44<00:00,  1.45it/s]\n",
            "100%|██████████| 23/23 [00:14<00:00,  1.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Train Loss: 1.1247, Bleu: 0.2013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(losses, marker='o', linestyle='-', color='b', label='Loss')\n",
        "plt.title(\"Loss Curve\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Loss\", fontsize=12)\n",
        "plt.legend(fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OwbPHFo8tAwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(bleus, marker='o', linestyle='-', color='b', label='Loss')\n",
        "plt.title(\"Bleu Curve\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Bleu\", fontsize=12)\n",
        "plt.legend(fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lHw6KUAFtLpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TextDataset(test_df, tokenizer)\n",
        "test_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "evaluate_bleu(test_loader, encoder, decoder)"
      ],
      "metadata": {
        "id": "ml7PHhfv0-fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tokenizer([sentence], max_length=MAX_LENGTH, padding=\"max_length\", truncation=True, return_tensors=\"pt\")[\"input_ids\"].squeeze().to(device)\n",
        "        input_tensor = input_tensor.unsqueeze(0)\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_INDEX:\n",
        "                decoded_words.append('</s>')\n",
        "                break\n",
        "            decoded_words.append(tokenizer.decode(idx))\n",
        "    return decoded_words, decoder_attn\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = test_df.sample()\n",
        "        print('>', pair['source'].iloc[0], end='')\n",
        "        print('=', pair['target'].iloc[0])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair['source'].iloc[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "QwyGyqx-YFUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder, decoder, n=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq7QuQbOm74H",
        "outputId": "87ff14cb-cca3-4a3b-9088-aa6673b3285e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> u culd try researching it on google or try bestbuy and blockbuster\n",
            "= ['You could try researching it on Google.  You could also try Best Buy and Blockbuster.', 'Take a look at either Google, Best Buy, or Blockbuster.', 'You might consider a research attempt on Google or explore Best Buy and Blockbuster.', 'You could research it on Google, or try a store like Best Buy or Blockbuster.']\n",
            "< <s> You could try downloading it on Google or Soul ter from Block bus ter . </s>\n",
            "\n",
            "> and the blonde in the boat says, this is, like, a sea of wheat, duh.\n",
            "= ['The blonde who was in the boat stated that it was \"like a sea of wheat\".', 'The blonde in the boat calls this a sea of wheat.', 'The blonde in the boat then replies, \"This is a sea of wheat!\"', 'And the reply from the blonde in the boat is \"this is like a sea of wheat.\"']\n",
            "< <s> The blonde in the boat , and this is the one who keep in the boat ,  a picture of  a rifle . </s>\n",
            "\n",
            "> name your favorite movie actress or actor\n",
            "= ['Name your favorite movie actress or actor.', 'Would you name your favorite film actress or actor?', 'Name your favorite actor or actress who stars in movies.', 'Could you name your favorite film actor or actress?']\n",
            "< <s> Which you are  a movie , or actor . </s>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uKVb4851ouG-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}